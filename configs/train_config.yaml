# Training Configuration for Deepfake Detection (target: >90% F1/AUC/accuracy)
# ===========================================================================
# Uses ALL datasets (Celeb-DF + FaceForensics) when available.
# Run: make extract-faces  (to extract FaceForensics faces) before training.

model:
  backbone: "efficientnet_b4"   # Options: efficientnet_b3, efficientnet_b4, efficientnet_b5
  pretrained: true              # Use ImageNet pretrained weights
  dropout: 0.4                  # Slightly lower for better generalization
  hidden_size: 512              # Hidden layer size

training:
  epochs: 50                    # More epochs for >90% metrics
  batch_size: 32                # Batch size (reduce to 24 if OOM)
  learning_rate: 0.00008        # Slightly lower LR for stability
  weight_decay: 0.00002         # Stronger L2 regularization
  warmup_epochs: 3              # Longer warmup
  grad_accumulation: 2          # Effective batch = batch_size * grad_accumulation
  mixed_precision: true         # FP16 training for memory efficiency
  early_stop_patience: 10       # Allow more epochs before early stopping
  min_delta: 0.0005             # Minimum improvement to reset patience

data:
  datasets_dir: "/app/datasets"   # Directory containing datasets
  image_size: 380                 # Input image size (380 for EfficientNet-B4)
  num_workers: 4                  # Data loading workers
  balance_classes: true           # Balance real/fake samples during training

loss:
  type: "combined"              # Options: bce, focal, combined
  bce_weight: 0.5               # Weight for BCE loss
  focal_weight: 0.5             # Weight for Focal loss
  focal_alpha: 0.25             # Focal loss alpha (class balance)
  focal_gamma: 2.0              # Focal loss gamma (focusing parameter)

checkpoints:
  save_dir: "/app/checkpoints"  # Directory to save checkpoints
  save_every: 1                 # Save every N epochs
  keep_best: 3                  # Keep only N best checkpoints

logging:
  log_dir: "/app/logs"          # TensorBoard log directory
  log_every: 50                 # Log every N batches

# GPU-specific configurations
# ---------------------------
# RTX 3060/3070 (8-12GB): batch_size=24, grad_accumulation=3
# RTX 3080/3090 (12-24GB): batch_size=32, grad_accumulation=2
# RTX 4090 / A100 (24GB+): batch_size=48, grad_accumulation=2
