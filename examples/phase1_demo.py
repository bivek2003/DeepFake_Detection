#!/usr/bin/env python3
"""
Phase 1 Demo: Deepfake Detection Data Pipeline

Demonstrates the complete Phase 1 functionality including:
- Dataset management and organization
- Video processing with face detection
- Audio feature extraction and analysis  
- PyTorch data pipeline creation
- Configuration management
- System utilities

Usage:
    python examples/phase1_demo.py

Author: Your Name
"""

import sys
import os
from pathlib import Path

# Add src to path for imports
sys.path.insert(0, str(Path(__file__).parent.parent / "src"))

from deepfake_detector.data import (
    DatasetManager, 
    VideoProcessor, 
    AudioProcessor,
    DataPipelineManager
)
from deepfake_detector.utils import (
    ConfigManager,
    LoggingSetup, 
    DeviceManager,
    SystemUtils,
    FileUtils,
    create_default_config_file
)

import logging
import numpy as np
import torch


def demonstrate_dataset_management():
    """Demo dataset management capabilities"""
    print("\n" + "="*60)
    print("üóÇÔ∏è  DATASET MANAGEMENT DEMO")
    print("="*60)
    
    # Initialize dataset manager
    dm = DatasetManager("./datasets")
    
    # Show available datasets
    print("\nüìã Available Deepfake Datasets:")
    dm.print_dataset_info(detailed=True)
    
    # Create sample dataset
    sample_dir = dm.create_sample_dataset("demo_data", video_count=5, audio_count=5)
    print(f"\nüìÅ Created sample dataset at: {sample_dir}")
    
    # List local datasets
    local_datasets = dm.list_local_datasets()
    print(f"\nüíæ Local Datasets Found: {len(local_datasets)}")
    for dataset in local_datasets:
        verification = dataset["verification"]
        print(f"  ‚Ä¢ {dataset['id']}: {verification['file_count']} files, "
              f"{verification['total_size_mb']:.1f} MB")
    
    return sample_dir


def demonstrate_video_processing():
    """Demo video processing capabilities"""
    print("\n" + "="*60)
    print("üé¨ VIDEO PROCESSING DEMO") 
    print("="*60)
    
    # Initialize video processor
    video_processor = VideoProcessor(
        target_size=(224, 224),
        max_frames=10,
        frame_interval=1
    )
    
    print(f"üìπ Video Processor Configuration:")
    print(f"  ‚Ä¢ Target size: {video_processor.target_size}")
    print(f"  ‚Ä¢ Max frames: {video_processor.max_frames}")
    print(f"  ‚Ä¢ Face detector: {video_processor.face_detector.backend}")
    print(f"  ‚Ä¢ Confidence threshold: {video_processor.face_detector.confidence_threshold}")
    
    # Demo capabilities (would work with real video files)
    print(f"\nüîß Processing Capabilities:")
    print(f"  ‚Ä¢ Face detection with OpenCV Haar cascades")
    print(f"  ‚Ä¢ Automatic face cropping and alignment") 
    print(f"  ‚Ä¢ Quality analysis (resolution, FPS, face detection rate)")
    print(f"  ‚Ä¢ Batch processing with parallel execution")
    print(f"  ‚Ä¢ Data augmentation pipeline")
    
    # Show example transforms
    from deepfake_detector.data.data_pipeline import VideoAugmentations
    train_transforms = VideoAugmentations.get_train_transforms()
    print(f"\nüé® Augmentation Pipeline:")
    print(f"  ‚Ä¢ Transforms: {len(train_transforms.transforms)} operations")
    print(f"  ‚Ä¢ Random flip, color jitter, rotation, affine")
    print(f"  ‚Ä¢ ImageNet normalization")
    
    return video_processor


def demonstrate_audio_processing():
    """Demo audio processing capabilities"""
    print("\n" + "="*60)
    print("üéµ AUDIO PROCESSING DEMO")
    print("="*60)
    
    # Initialize audio processor
    audio_processor = AudioProcessor(
        sample_rate=16000,
        duration=3.0,
        n_mfcc=13,
        n_mels=128
    )
    
    print(f"üé§ Audio Processor Configuration:")
    print(f"  ‚Ä¢ Sample rate: {audio_processor.sample_rate} Hz")
    print(f"  ‚Ä¢ Duration: {audio_processor.duration} seconds")
    print(f"  ‚Ä¢ MFCC coefficients: {audio_processor.n_mfcc}")
    print(f"  ‚Ä¢ Mel bands: {audio_processor.n_mels}")
    print(f"  ‚Ä¢ Hop length: {audio_processor.hop_length}")
    
    # Demo feature extraction with synthetic audio
    print(f"\nüîç Feature Extraction Demo:")
    synthetic_audio = np.random.randn(audio_processor.n_samples) * 0.1
    features = audio_processor.extract_features(synthetic_audio)
    
    print(f"  ‚Ä¢ MFCC shape: {features.mfcc.shape}")
    print(f"  ‚Ä¢ Mel spectrogram shape: {features.mel_spectrogram.shape}")
    print(f"  ‚Ä¢ Spectral centroid shape: {features.spectral_centroid.shape}")
    print(f"  ‚Ä¢ Chroma shape: {features.chroma.shape}")
    print(f"  ‚Ä¢ Tempo: {features.tempo:.2f} BPM")
    
    # Demo augmentations
    print(f"\nüé® Audio Augmentation Demo:")
    augmented = audio_processor.create_augmentations(synthetic_audio, num_augmentations=3)
    print(f"  ‚Ä¢ Generated {len(augmented)} augmented versions")
    print(f"  ‚Ä¢ Augmentations: noise, time stretch, pitch shift, time shift, volume")
    
    # Demo voice activity detection
    voice_activity = audio_processor.detect_voice_activity(synthetic_audio)
    voice_ratio = np.sum(voice_activity) / len(voice_activity)
    print(f"  ‚Ä¢ Voice activity ratio: {voice_ratio:.2%}")
    
    return audio_processor


def demonstrate_data_pipeline():
    """Demo PyTorch data pipeline"""
    print("\n" + "="*60)
    print("üîó PYTORCH DATA PIPELINE DEMO")
    print("="*60)
    
    # Initialize pipeline manager
    pipeline = DataPipelineManager("./datasets", "./preprocessed")
    
    # Create sample data
    print(f"üìä Creating Sample Data:")
    sample_data = pipeline.create_sample_dataset()
    
    video_paths = sample_data["video_paths"]
    video_labels = sample_data["video_labels"] 
    audio_paths = sample_data["audio_paths"]
    audio_labels = sample_data["audio_labels"]
    
    print(f"  ‚Ä¢ Video samples: {len(video_paths)}")
    print(f"  ‚Ä¢ Audio samples: {len(audio_paths)}")
    print(f"  ‚Ä¢ Class distribution: {np.bincount(video_labels)}")
    
    # Demo data splitting
    from deepfake_detector.data.data_pipeline import DataSplitter
    splitter = DataSplitter(test_size=0.2, val_size=0.1, random_state=42)
    
    print(f"\nüìà Data Splitting Demo:")
    video_split = splitter.split_data(video_paths, video_labels)
    splitter.print_split_info(video_split)
    
    # Demo dataset creation (conceptual - would need real files)
    print(f"\nüéØ PyTorch Integration:")
    print(f"  ‚Ä¢ DeepfakeVideoDataset: Face crops ‚Üí 224√ó224 RGB tensors")
    print(f"  ‚Ä¢ DeepfakeAudioDataset: Audio ‚Üí MFCC/Mel/Raw features")
    print(f"  ‚Ä¢ Stratified splitting with class balance")
    print(f"  ‚Ä¢ Configurable batch size and augmentations")
    print(f"  ‚Ä¢ GPU acceleration with pin_memory")
    
    # Show dataloader configuration
    print(f"\n‚öôÔ∏è DataLoader Configuration:")
    print(f"  ‚Ä¢ Batch size: 32")
    print(f"  ‚Ä¢ Num workers: 4")
    print(f"  ‚Ä¢ Pin memory: {torch.cuda.is_available()}")
    print(f"  ‚Ä¢ Shuffle: True (train), False (val/test)")
    
    return pipeline


def demonstrate_configuration():
    """Demo configuration management"""
    print("\n" + "="*60)
    print("‚öôÔ∏è  CONFIGURATION MANAGEMENT DEMO")
    print("="*60)
    
    # Create and load configuration
    config_manager = ConfigManager()
    config = config_manager.get_config()
    
    print(f"üìù Configuration Settings:")
    print(f"  ‚Ä¢ Data root: {config.data.data_root}")
    print(f"  ‚Ä¢ Video target size: {config.data.video_target_size}")
    print(f"  ‚Ä¢ Audio sample rate: {config.data.audio_sample_rate} Hz")
    print(f"  ‚Ä¢ Test size: {config.data.test_size}")
    print(f"  ‚Ä¢ Batch size: {config.training.batch_size}")
    print(f"  ‚Ä¢ Learning rate: {config.training.learning_rate}")
    
    # Create default config file
    config_path = create_default_config_file("demo_config.yaml")
    print(f"\nüìÑ Created config file: {config_path}")
    
    # Setup logging
    logger = LoggingSetup.setup_logging(config.logging)
    logger.info("Logging system initialized for demo")
    
    return config_manager


def demonstrate_system_utilities():
    """Demo system utilities"""
    print("\n" + "="*60)
    print("üîß SYSTEM UTILITIES DEMO")
    print("="*60)
    
    # System information
    system_info = SystemUtils.get_system_info()
    print(f"üíª System Information:")
    print(f"  ‚Ä¢ Platform: {system_info['platform']}")
    print(f"  ‚Ä¢ Python: {system_info['python_version']}")
    print(f"  ‚Ä¢ CPU cores: {system_info['cpu_count']}")
    print(f"  ‚Ä¢ Memory: {system_info['memory_total_gb']:.1f} GB")
    print(f"  ‚Ä¢ Disk space: {system_info['disk_usage']['free_gb']:.1f} GB free")
    
    # Device information
    device = DeviceManager.get_device("auto")
    device_info = DeviceManager.get_device_info()
    print(f"\nüñ•Ô∏è  Device Information:")
    print(f"  ‚Ä¢ Selected device: {device}")
    print(f"  ‚Ä¢ CUDA available: {device_info['cuda_available']}")
    if device_info['cuda_available']:
        print(f"  ‚Ä¢ GPU count: {device_info['device_count']}")
        for i, gpu in enumerate(device_info['devices']):
            print(f"    - GPU {i}: {gpu['name']}")
            print(f"      Memory: {gpu['total_memory'] / 1e9:.1f} GB")
    
    # Dependency check
    deps = SystemUtils.check_dependencies()
    print(f"\nüì¶ Dependency Status:")
    for name, status in deps.items():
        print(f"  ‚Ä¢ {name}: {status}")
    
    # File utilities demo
    print(f"\nüìÅ File Utilities:")
    video_exts = FileUtils.get_video_extensions()
    audio_exts = FileUtils.get_audio_extensions()
    print(f"  ‚Ä¢ Video formats: {', '.join(video_exts[:5])}... ({len(video_exts)} total)")
    print(f"  ‚Ä¢ Audio formats: {', '.join(audio_exts[:5])}... ({len(audio_exts)} total)")


def demonstrate_performance_metrics():
    """Demo performance and metrics"""
    print("\n" + "="*60)
    print("üìä PERFORMANCE & METRICS DEMO")
    print("="*60)
    
    from deepfake_detector.utils import MetricsCalculator, ProgressTracker
    import time
    
    # Simulate some predictions and targets
    np.random.seed(42)
    predictions = np.random.randint(0, 2, 1000)
    targets = np.random.randint(0, 2, 1000)
    
    # Calculate metrics
    metrics = MetricsCalculator.calculate_classification_report(predictions, targets)
    
    print(f"üéØ Classification Metrics:")
    print(f"  ‚Ä¢ Accuracy: {metrics['accuracy']:.3f}")
    print(f"  ‚Ä¢ Precision (Real): {metrics['precision_per_class'][0]:.3f}")
    print(f"  ‚Ä¢ Precision (Fake): {metrics['precision_per_class'][1]:.3f}")
    print(f"  ‚Ä¢ Recall (Real): {metrics['recall_per_class'][0]:.3f}")
    print(f"  ‚Ä¢ Recall (Fake): {metrics['recall_per_class'][1]:.3f}")
    print(f"  ‚Ä¢ F1 Macro: {metrics['f1_macro']:.3f}")
    
    # Progress tracking demo
    print(f"\n‚è±Ô∏è  Progress Tracking Demo:")
    tracker = ProgressTracker(100, "Processing samples")
    for i in range(0, 101, 20):
        tracker.update(20, f"Batch {i//20 + 1}/5")
        time.sleep(0.1)  # Simulate work
    tracker.finish("All samples processed successfully")
    
    # Performance estimates
    print(f"\nüöÄ Expected Performance (Phase 1):")
    print(f"  ‚Ä¢ Face detection: ~30 FPS (CPU), ~100 FPS (GPU)")
    print(f"  ‚Ä¢ Audio MFCC extraction: Real-time (3s audio in <0.1s)")
    print(f"  ‚Ä¢ Video batch processing: 4√ó speedup with parallel workers")
    print(f"  ‚Ä¢ Memory usage: ~2GB for 10K video samples (preloaded)")


def main():
    """Run complete Phase 1 demonstration"""
    print("üé≠ DEEPFAKE DETECTION SYSTEM - PHASE 1 DEMO")
    print("=" * 80)
    print("Comprehensive demonstration of data pipeline capabilities")
    print("Ready for model development (Phase 2)")
    print("=" * 80)
    
    try:
        # Run all demonstrations
        sample_dir = demonstrate_dataset_management()
        video_processor = demonstrate_video_processing()
        audio_processor = demonstrate_audio_processing()
        pipeline = demonstrate_data_pipeline()
        config_manager = demonstrate_configuration()
        demonstrate_system_utilities()
        demonstrate_performance_metrics()
        
        # Summary
        print("\n" + "="*60)
        print("‚úÖ PHASE 1 DEMONSTRATION COMPLETE")
        print("="*60)
        print("üéØ What we've accomplished:")
        print("  ‚úÖ Dataset management for 7+ major deepfake datasets")
        print("  ‚úÖ Video processing with face detection and extraction")
        print("  ‚úÖ Audio processing with comprehensive feature extraction")
        print("  ‚úÖ PyTorch data pipeline with efficient loading")
        print("  ‚úÖ Configuration management and system utilities")
        print("  ‚úÖ Professional logging and error handling")
        print("  ‚úÖ Scalable architecture ready for production")
        
        print(f"\nüöÄ Ready for Phase 2:")
        print("  ‚Ä¢ Model development (EfficientNet, Vision Transformers)")
        print("  ‚Ä¢ Training pipelines and evaluation metrics")
        print("  ‚Ä¢ Cross-dataset validation")
        print("  ‚Ä¢ Ensemble methods")
        
        print(f"\nüìÅ Generated Files:")
        print(f"  ‚Ä¢ Sample dataset: {sample_dir}")
        print(f"  ‚Ä¢ Configuration: demo_config.yaml")
        print(f"  ‚Ä¢ Logs: ./logs/deepfake_detector.log")
        
        print(f"\nüéâ System Status: READY FOR DEVELOPMENT")
        
    except Exception as e:
        logging.error(f"Demo failed with error: {e}")
        print(f"‚ùå Demo failed: {e}")
        return 1
    
    return 0


if __name__ == "__main__":
    exit_code = main()
    sys.exit(exit_code)
